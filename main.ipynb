{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading MNIST data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print('X_train shape', X_train.shape, 'X_test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing randomly some images in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "plt.figure(figsize = (12,5))\n",
    "for i in range(8):\n",
    "  ind = random.randint(0, len(X_train))\n",
    "  plt.subplot(240+1+i)\n",
    "  plt.imshow(X_train[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data:\n",
    "\n",
    "This task includes the following steps:\n",
    "\n",
    "Reshape images into the required size of Keras\n",
    "Convert integer values into float values\n",
    "Normalize data\n",
    "One-hot encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "def preprocess_data(X_train, y_train, X_test, y_test):\n",
    "  # reshape images to the required size of Keras\n",
    "  X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "  X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "  \n",
    "  # convert image values from integers to floats\n",
    "  X_train = X_train.astype('float32')\n",
    "  X_test = X_test.astype('float32')\n",
    "  \n",
    "  # normalization\n",
    "  X_train = X_train/255.0\n",
    "  X_test_norm = X_test/255.0\n",
    "  \n",
    "  # One-hot encoding label \n",
    "  y_train = to_categorical(y_train)\n",
    "  y_test = to_categorical(y_test)\n",
    "  \n",
    "  return X_train, y_train, X_test, y_test\n",
    "\n",
    "# (X_train, y_train, X_test, y_test) = preprocess_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building LeNet5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "# metrics \n",
    "from keras.metrics import categorical_crossentropy\n",
    "# optimization method\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def LeNet():\n",
    "  model = Sequential()\n",
    "  \n",
    "  # Convolutional layer  \n",
    "  model.add(Conv2D(filters = 6, kernel_size = (5,5), padding = 'same', \n",
    "                   activation = 'relu', input_shape = (28,28,1)))\n",
    "  \n",
    "  # Max-pooing layer with pooling window size is 2x2\n",
    "  model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "  \n",
    "  # Convolutional layer \n",
    "  model.add(Conv2D(filters = 16, kernel_size = (5,5), activation = 'relu'))\n",
    "  \n",
    "  # Max-pooling layer \n",
    "  model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "  \n",
    "  # Flatten layer \n",
    "  model.add(Flatten())\n",
    "  \n",
    "  # The first fully connected layer \n",
    "  model.add(Dense(120, activation = 'relu'))\n",
    "  \n",
    "  # The output layer  \n",
    "  model.add(Dense(10, activation = 'softmax'))\n",
    "  \n",
    "  # compile the model with a loss function, a metric and an optimizer function\n",
    "  # In this case, the loss function is categorical crossentropy, \n",
    "  # we use Stochastic Gradient Descent (SGD) method with learning rate lr = 0.01 \n",
    "  # to optimize the loss function\n",
    "  # metric: accuracy \n",
    "  \n",
    "  opt = SGD(learning_rate = 0.01)\n",
    "  model.compile(loss = categorical_crossentropy, \n",
    "                optimizer = opt, \n",
    "                metrics = ['accuracy']) \n",
    "                \n",
    "  return model\n",
    "\n",
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LeNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_history(history):\n",
    "  plt.figure(figsize = (10,6))\n",
    "  plt.plot(history.history['accuracy'], color = 'blue', label = 'train')\n",
    "  plt.plot(history.history['val_accuracy'], color = 'red', label = 'val')\n",
    "  plt.legend()\n",
    "  plt.title('Accuracy')\n",
    "  plt.show()\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, epochs = 50, batch_size = 468):\n",
    "  # Rescaling all training and testing data\n",
    "  X_train, y_train, X_test, y_test = preprocess_data(X_train, y_train, X_test, y_test)\n",
    "  # Fitting the model on the training set\n",
    "  history = model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, \n",
    "                      steps_per_epoch = X_train.shape[0]//batch_size, \n",
    "                      validation_data = (X_test, y_test), \n",
    "                      validation_steps = X_test.shape[0]//batch_size, verbose = 1)\n",
    "  # evaluating the model\n",
    "  _, acc = model.evaluate(X_test, y_test, verbose = 1)\n",
    "  print('%.3f' % (acc * 100.0))\n",
    "  summary_history(history)\n",
    "\n",
    "train_model(model, X_train, y_train, X_test, y_test, 4, 468)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# predict labels for the test set\n",
    "y_test_pred = []\n",
    "for i in range(len(X_test)):\n",
    "  img = X_test[i]\n",
    "  img = img.reshape(1,28,28,1)\n",
    "  img = img.astype('float32')\n",
    "  img = img/255.0\n",
    "  # one-hot vector output\n",
    "  vec_p = model.predict(img)\n",
    "  # determine the label corresponding to vector vec_p\n",
    "  y_p = np.argmax(vec_p)\n",
    "  y_test_pred.append(y_p)\n",
    "  \n",
    "# convert y_test_pred from list to array\n",
    "y_test_pred = np.asarray(y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
