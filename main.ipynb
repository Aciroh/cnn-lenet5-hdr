{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading MNIST data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape (60000, 28, 28) X_test shape (10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print('X_train shape', X_train.shape, 'X_test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing randomly some images in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "plt.figure(figsize = (12,5))\n",
    "for i in range(8):\n",
    "  ind = random.randint(0, len(X_train))\n",
    "  plt.subplot(240+1+i)\n",
    "  plt.imshow(X_train[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data:\n",
    "\n",
    "This task includes the following steps:\n",
    "\n",
    "Reshape images into the required size of Keras\n",
    "Convert integer values into float values\n",
    "Normalize data\n",
    "One-hot encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "def preprocess_data(X_train, y_train, X_test, y_test):\n",
    "  # reshape images to the required size of Keras\n",
    "  X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "  X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "  \n",
    "  # convert image values from integers to floats\n",
    "  X_train = X_train.astype('float32')\n",
    "  X_test = X_test.astype('float32')\n",
    "  \n",
    "  # normalization\n",
    "  X_train = X_train/255.0\n",
    "  X_test_norm = X_test/255.0\n",
    "  \n",
    "  # One-hot encoding label \n",
    "  y_train = to_categorical(y_train)\n",
    "  y_test = to_categorical(y_test)\n",
    "  \n",
    "  return X_train, y_train, X_test, y_test\n",
    "\n",
    "(X_train, y_train, X_test, y_test) = preprocess_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building LeNet5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/horatiu/.pyenv/versions/3.10.13/envs/cnn-lenet5-hdr/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.01}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     41\u001b[0m   model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m categorical_crossentropy, \n\u001b[1;32m     42\u001b[0m                 optimizer \u001b[38;5;241m=\u001b[39m opt, \n\u001b[1;32m     43\u001b[0m                 metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[1;32m     45\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 47\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLeNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m, in \u001b[0;36mLeNet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m10\u001b[39m, activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# compile the model with a loss function, a metric and an optimizer function\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# In this case, the loss function is categorical crossentropy, \u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# we use Stochastic Gradient Descent (SGD) method with learning rate lr = 0.01 \u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# to optimize the loss function\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# metric: accuracy \u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m opt \u001b[38;5;241m=\u001b[39m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m categorical_crossentropy, \n\u001b[1;32m     42\u001b[0m               optimizer \u001b[38;5;241m=\u001b[39m opt, \n\u001b[1;32m     43\u001b[0m               metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/cnn-lenet5-hdr/lib/python3.10/site-packages/keras/src/optimizers/sgd.py:60\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[0;34m(self, learning_rate, momentum, nesterov, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     59\u001b[0m ):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclipvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclipvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_clipnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_ema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_ema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mema_overwrite_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_scale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_scale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(momentum, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m momentum \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m momentum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`momentum` must be a float between [0, 1].\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/cnn-lenet5-hdr/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:22\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/cnn-lenet5-hdr/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:37\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[0;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 0.01}"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "# metrics \n",
    "from keras.metrics import categorical_crossentropy\n",
    "# optimization method\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def LeNet():\n",
    "  model = Sequential()\n",
    "  \n",
    "  # Convolutional layer  \n",
    "  model.add(Conv2D(filters = 6, kernel_size = (5,5), padding = 'same', \n",
    "                   activation = 'relu', input_shape = (28,28,1)))\n",
    "  \n",
    "  # Max-pooing layer with pooling window size is 2x2\n",
    "  model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "  \n",
    "  # Convolutional layer \n",
    "  model.add(Conv2D(filters = 16, kernel_size = (5,5), activation = 'relu'))\n",
    "  \n",
    "  # Max-pooling layer \n",
    "  model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "  \n",
    "  # Flatten layer \n",
    "  model.add(Flatten())\n",
    "  \n",
    "  # The first fully connected layer \n",
    "  model.add(Dense(120, activation = 'relu'))\n",
    "  \n",
    "  # The output layer  \n",
    "  model.add(Dense(10, activation = 'softmax'))\n",
    "  \n",
    "  # compile the model with a loss function, a metric and an optimizer function\n",
    "  # In this case, the loss function is categorical crossentropy, \n",
    "  # we use Stochastic Gradient Descent (SGD) method with learning rate lr = 0.01 \n",
    "  # to optimize the loss function\n",
    "  # metric: accuracy \n",
    "  \n",
    "  opt = SGD(learning_rate = 0.01)\n",
    "  model.compile(loss = categorical_crossentropy, \n",
    "                optimizer = opt, \n",
    "                metrics = ['accuracy']) \n",
    "                \n",
    "  return model\n",
    "\n",
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training LeNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_history(history):\n",
    "  plt.figure(figsize = (10,6))\n",
    "  plt.plot(history.history['accuracy'], color = 'blue', label = 'train')\n",
    "  plt.plot(history.history['val_accuracy'], color = 'red', label = 'val')\n",
    "  plt.legend()\n",
    "  plt.title('Accuracy')\n",
    "  plt.show()\n",
    "\n",
    "def train_model(model, X_train, y_train, X_test, y_test, epochs = 50, batch_size = 128):\n",
    "  # Rescaling all training and testing data\n",
    "  X_train, y_train, X_test, y_test = preprocess_data(X_train, y_train, X_test, y_test)\n",
    "  # Fitting the model on the training set\n",
    "  history = model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size, \n",
    "                      steps_per_epoch = X_train.shape[0]//batch_size, \n",
    "                      validation_data = (X_test, y_test), \n",
    "                      validation_steps = X_test.shape[0]//batch_size, verbose = 1)\n",
    "  # evaluating the model\n",
    "  _, acc = model.evaluate(X_test, y_test, verbose = 1)\n",
    "  print('%.3f' % (acc * 100.0))\n",
    "  summary_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# predict labels for the test set\n",
    "y_test_pred = []\n",
    "for i in range(len(X_test)):\n",
    "  img = X_test[i]\n",
    "  img = img.reshape(1,28,28,1)\n",
    "  img = img.astype('float32')\n",
    "  img = img/255.0\n",
    "  # one-hot vector output\n",
    "  vec_p = LeNet_model.predict(img)\n",
    "  # determine the label corresponding to vector vec_p\n",
    "  y_p = np.argmax(vec_p)\n",
    "  y_test_pred.append(y_p)\n",
    "  \n",
    "# convert y_test_pred from list to array\n",
    "y_test_pred = np.asarray(y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
